{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知机模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、感知机是根据输入特征实例x进行二分类的线性分类模型：\n",
    "$$\n",
    "f(x)=\\operatorname{sign}(w\\cdot x+b)\n",
    "$$\n",
    "其中，$\\operatorname{sign}$是符号函数，即\n",
    "$$\n",
    "\\operatorname{sign}(x)=\n",
    "\\begin{cases}\n",
    "+1, x>=0\\\\\n",
    "-2, x<0\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "w为分离超平面的法向量，b为分离超平面的截距\n",
    "\n",
    "2、数据集的线性可分性：\n",
    "给定数据集\n",
    "$$\n",
    "T=\\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\\}\n",
    "$$\n",
    "如果存在某个超平面S\n",
    "$$\n",
    "w\\cdot x+b=0\n",
    "$$\n",
    "使得$y_{i}(w\\cdot x_{i}+b)>0$,则称数据集T为线性可分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数间隔损失函数\n",
    "\n",
    "任意一点$x_{0}$到超平面S的距离为\n",
    "$$\n",
    "\\frac{1}{\\Arrowvert w \\Arrowvert}\\bracevert w\\cdot x_{0}+b\\bracevert\n",
    "$$\n",
    "对于误分类的数据$(x_{i}, y_{i})$来说，\n",
    "$$\n",
    "-y_{i}w\\cdot x_{i}+b>0\n",
    "$$\n",
    "因此，误分类点$x_{i}$到超平面的距离为\n",
    "$$\n",
    "-\\frac{1}{\\Arrowvert w \\Arrowvert}y_{i}(w\\cdot x_{i}+b)\n",
    "$$\n",
    "这样，对于误分类点集合M，所有误分类点到超平面的距离之和为\n",
    "$$\n",
    "-\\frac{1}{\\Arrowvert w \\Arrowvert}\\sum_{x_{i}\\in M} y_{i}(w\\cdot x_{i}+b)\n",
    "$$\n",
    "不考虑$\\frac{1}{\\Arrowvert w \\Arrowvert}$，得到损失函数\n",
    "$$\n",
    "L(w,b)=-\\sum_{x_{i}\\in M} y_{i}(w\\cdot x_{i}+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.\n",
    "\n",
    "为什么这里不考虑$\\frac{1}{\\Arrowvert w \\Arrowvert}$?\n",
    "\n",
    "A.\n",
    "\n",
    "1.超平面由法向量w和截距b确定，w的大小并不影响w的方向，$\\frac{1}{\\Arrowvert w \\Arrowvert}$的大小并不影响分类的结果（这里也可以看作将$\\Arrowvert w \\Arrowvert$视为1）。\n",
    "\n",
    "2.对于线性可分数据集，最终的损失函数值为0，$\\frac{1}{\\Arrowvert w \\Arrowvert}$并不影响分类的最总结果。\n",
    "\n",
    "3.采用梯度下降等算法时，不考虑$\\frac{1}{\\Arrowvert w \\Arrowvert}$可以减少运算量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机学习方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、感知机的学习目标为最小化损失函数，即\n",
    "$$\n",
    "\\min_{w,b}L(w,b)=-\\sum_{x_{i}\\in M} y_{i}(w\\cdot x_{i}+b)\n",
    "$$\n",
    "2、采用梯度下降法进行学习\n",
    "\n",
    "2.1、梯度下降法\n",
    "由泰勒公式：\n",
    "$$\n",
    "f(x + \\Delta x)=f(x)+\\sum_{i=1}^{\\infty}\\frac{1}{i!}f^{(i)}(x)(\\Delta x)^i\n",
    "$$\n",
    "可得$f(x + \\Delta x) = f(x)+f'(x)\\Delta x + O(\\Delta x)$，当$\\Delta x*f'(x)>0$ 且 $\\Delta x$值较小时,$f(x + \\Delta x) > f(x)$,由此可构建一个递增的序列达到极大值，同理可达到极小值。\n",
    "\n",
    "将该结论推广至高维情况，\n",
    "$$\n",
    "f(x + \\Delta x)=f(x) + [\\nabla f(x)]^T\\cdot \\Delta x + O(\\Delta x)\n",
    "$$\n",
    "在$\\Delta x$取值较小时，可以忽略$O(\\Delta x)$,得到\n",
    "$$\n",
    "f(x + \\Delta x)-f(x)=[\\nabla f(x)]^T\\cdot \\Delta x=\\Arrowvert \\nabla f(x)\\Arrowvert \\Arrowvert\\Delta x\\Arrowvert \\operatorname{cos}\\theta\n",
    "$$\n",
    "其中$\\operatorname{cos}\\theta$为 $\\nabla f(x)$和$\\Delta x$夹角的余弦，当 $\\theta = \\pi$即增量方向与梯度方向相反时，下降速度最快。\n",
    "\n",
    "P.S.在讨论时，我们要求$\\Delta x$的值较小，在实现时一般添加一个较小的常数来进行限制称之为学习率。\n",
    "\n",
    "P.S.S在这里我们只对损失函数进行一阶泰勒展开，高阶泰勒展开也是理论上可行的。\n",
    "2.1.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
