{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机（SVM）\n",
    "## 线性可分支持向量机与硬间隔最大化\n",
    "### 线性可分支持向量机\n",
    "**学习的目标**是在特征空间中找到一个分离超平面，能将实例分到不同类。分离超平面对应于方程$w\\cdot x+b=0$,由法向量w与截距b决定，可用（w,b）表示。在训练集线性可分时，存在无穷多个超平面。线性可分支持向量机利用间隔最大化求最优分离超平面，此时，解是唯一的。\n",
    "\n",
    "**线性可分支持向量机**给定线性可分训练数据集，通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为\n",
    "$$\n",
    "w^*\\cdot x+b^*=0\n",
    "$$\n",
    "以及相应的分类决策函数\n",
    "$$\n",
    "f(x)=sign(w^*\\cdot x+b^*)\n",
    "$$\n",
    "称为线性可分支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数间隔与几何间隔\n",
    "在超平面$w\\cdot x+b=0$确定的情况下，$|w\\cdot x+b|$能够表示点x到超平面的距离，而$w\\cdot x+b$的符号与y的符号可以表示分类正确与否，因此可以用$y(w\\cdot x+b)$表示分类的正确性与确信度。\n",
    "\n",
    "**函数间隔**对于给定的训练数据集T和超平面(w, b)，定义超平面(w, b)关于样本点$(x_i, y_i)$的函数间隔为\n",
    "$$\n",
    "\\hat{\\gamma_i}=y_i(w\\cdot x_i+b)\n",
    "$$\n",
    "定义超平面(w ,b)关于训练数据集T的函数间隔为超平面(w, b)关于T中所有样本点$(x_i, y_i)$的函数间隔最小值，即\n",
    "$$\n",
    "\\hat{\\gamma}=\\min_{i=1,2,\\cdots,N}\\hat{\\gamma_i}\n",
    "$$\n",
    "\n",
    "\n",
    "在选择超平面时，只要成倍改变w，b，超平面并没有改变但函数间隔会成倍增加，因此，我们需要对超平面法向量w进行某些约束，如规范化$\\bracevert\\bracevert w\\bracevert\\bracevert=1$,使得间隔是确定的，这时函数间隔称为几何间隔。\n",
    "\n",
    "**几何间隔**对于给定的训练数据集T和超平面(w, b)，定义超平面(w, b)关于样本点$(x_i, y_i)$的几何间隔为\n",
    "$$\n",
    "\\hat{\\gamma_i}=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})\n",
    "$$\n",
    "定义超平面(w ,b)关于训练数据集T的函数间隔为超平面(w, b)关于T中所有样本点$(x_i, y_i)$的函数间隔最小值，即\n",
    "$$\n",
    "\\hat{\\gamma}=\\min_{i=1,2,\\cdots,N}\\hat{\\gamma_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 间隔最大化\n",
    "间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类，也就是说，不仅将正负实例点分开，而且对最难分的实例点也有足够大的确信度将它们分开。\n",
    "\n",
    "#### 最大间隔分离超平面\n",
    "这个问题可以表示为下面的约束最优化问题：\n",
    "$$\n",
    "\\max_{w,b}\\quad\\gamma\\\\\n",
    "s.t.\\quad y_i\\left(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||}\\right)\\ge\\gamma, \\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "也可以表达为\n",
    "$$\n",
    "\\max_{w,b}\\quad\\frac{\\hat{\\gamma}}{||w||}\\\\\n",
    "s.t.\\quad y_i(w\\cdot x_i+b)\\ge\\hat{\\gamma},\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "函数间隔$\\hat{\\gamma}$的取值并不影响最优化问题的解，假设将w和b按比例改变为$\\lambda w$和$\\lambda b$，这时函数间隔为$\\lambda\\hat{\\gamma}$,也就是说函数间隔的大小并不影响问题的约束，因此，不妨取$\\hat{\\gamma}=1$。同时，注意到最大化$\\frac{1}{||w||}$等价于最小化$\\frac{1}{2}||w||^2$，于是得到如下最优化问题\n",
    "$$\n",
    "\\min_{w,b}\\quad\\frac{1}{2}||w||^2\\\\\n",
    "s.t.\\quad y_i(w\\cdot x_i+b)-1\\ge 0,\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "这是一个凸二次规划问题。\n",
    "\n",
    "**最大间隔法**\n",
    "\n",
    "输入：线性可分数据集$T=\\{(x_1,y_1), (x_2,y_2),\\cdots,(x_N,y_N)\\}$\n",
    "\n",
    "输出：最大间隔分离超平面和分类决策函数。\n",
    "\n",
    "(1)构造并求解约束最优化问题：\n",
    "$$\n",
    "\\min_{w,b}\\quad\\frac{1}{2}||w||^2\\\\\n",
    "s.t.\\quad y_i(w\\cdot x_i+b)-1\\ge 0,\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "求得最优解$w^*, b^*$。\n",
    "\n",
    "(2)由此得到分离超平面：\n",
    "$$\n",
    "w^*\\cdot x+b^*=0\n",
    "$$\n",
    "分类决策函数\n",
    "$$\n",
    "f(x)=sign(w^*\\cdot x+b^*)\n",
    "$$\n",
    "\n",
    "#### 支持向量与决策边界\n",
    "在线性可分的情况下，训练数据集的样本点中与超平面距离最近的样本点的实例称为**支持向量**。\n",
    "\n",
    "在决定分离超平面时，只有支持向量起作用，而其他实例点并不起作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对偶问题\n",
    "**拉格朗日函数**对于优化问题\n",
    "$$\n",
    "\\min_uf(u)\\\\\n",
    "s.t.g_i(u)\\le 0,\\quad i=1,2,\\cdots,m\\\\\n",
    "h_j(u)=0,\\quad j=1,2,\\cdots,n\n",
    "$$\n",
    "定义其拉格朗日函数为\n",
    "$$\n",
    "L(u,\\alpha,\\beta):=f(u)+\\sum_{i=1}^m\\alpha_ig_i(u)+\\sum_{j=1}^n\\beta_jh_j(u)\n",
    "$$\n",
    "其中$\\alpha_i\\ge0$\n",
    "\n",
    "则优化问题等价于\n",
    "$$\n",
    "\\min_u\\max_{\\alpha,\\beta}L(u,\\alpha,\\beta)\\\\\n",
    "s.t.\\alpha_i\\ge0,\\quad i=1,2,\\cdots,m\n",
    "$$\n",
    "**KKT条件**上式描述的优化问题在最值处必须满足如下条件\n",
    "\n",
    "(1)主问题可行：$g_i(u)\\le0,h_i(u)=0$\n",
    "\n",
    "(2)对偶问题可行：$\\alpha\\ge0$\n",
    "\n",
    "(3)互补松弛:$\\alpha_ig_i(u)=0$\n",
    "\n",
    "**对偶问题**优化问题的对偶问题定义为：\n",
    "$$\n",
    "\\max_{\\alpha,\\beta}\\min_uL(u,\\alpha,\\beta)\\\\\n",
    "s.t.\\alpha_i\\ge0,\\quad i=1,2,\\cdots,m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  学习的对偶算法\n",
    "我们可以得到优化问题的拉格朗日函数为\n",
    "$$\n",
    "L(w, b, \\alpha)=\\frac{1}{2}||w||^2+\\sum_{i=1}^N\\alpha_i(1-y_i(w\\cdot x_i+b))\\\\\n",
    "=\\frac{1}{2}||w||^2-\\sum_{i=1}^N\\alpha_iy_i(w\\cdot x_i+b)+\\sum_{i=1}^N\\alpha_i\n",
    "$$\n",
    "这个问题的对偶问题是\n",
    "$$\n",
    "\\max_\\alpha\\min_{w, b}L(w, b, \\alpha)\n",
    "$$\n",
    "\n",
    "(1)求$\\min_{w, b}L(w, b, \\alpha)$\n",
    "$$\n",
    "\\nabla_wL(w, b, \\alpha)=w-\\sum_{i=1}^N\\alpha_iy_ix_i=0\\\\\n",
    "\\nabla_bL(w, b, \\alpha)=\\sum_{i=1}^N\\alpha_iy_i=0\n",
    "$$\n",
    "得\n",
    "$$\n",
    "w=\\sum_{i=1}^N\\alpha_iy_ix_i\\\\\n",
    "\\sum_{i=1}^N\\alpha_iy_i=0\n",
    "$$\n",
    "将结果代入得\n",
    "$$\n",
    "L(w, b, \\alpha)=-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)+\\sum_{i=1}^N\\alpha_i\n",
    "$$\n",
    "\n",
    "(2)求对$\\alpha$的极大\n",
    "$$\n",
    "\\max_\\alpha-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)+\\sum_{i=1}^N\\alpha_i\\\\\n",
    "s.t.\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "\\alpha_i\\ge0,\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "\n",
    "\n",
    "**算法**\n",
    "输入：线性可分数据集$T=\\{(x_1,y_1), (x_2,y_2),\\cdots,(x_N,y_N)\\}$\n",
    "\n",
    "输出：最大间隔分离超平面和分类决策函数。\n",
    "\n",
    "(1)构造并解决约束最优化问题\n",
    "$$\n",
    "\\min_\\alpha\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i\\\\\n",
    "s.t.\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "\\alpha_i\\ge0,\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "(2)计算\n",
    "$$\n",
    "w^* = \\sum_{i=1}^N\\alpha_i^*y_ix_i\n",
    "$$\n",
    "并选择$\\alpha^*$的一个正分量$\\alpha_j^*>0$来计算\n",
    "$$\n",
    "b^*=\\sum_{i=1}^N\\alpha_i^*y_i(x_i\\cdot x_j)\n",
    "$$\n",
    "(3)得到分离超平面\n",
    "$$\n",
    "w^*\\cdot x+b^*=0\n",
    "$$\n",
    "分类决策函数\n",
    "$$\n",
    "f(x)=sign(w^*\\cdot x+b^*)\n",
    "$$\n",
    "### 支持向量\n",
    "对于对偶问题的解$\\alpha^*$中对应于$\\alpha^*_i$大于0的样本点$(x_i,y_i)$的实例$x_i$称为支持向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性支持向量机与软间隔最大化\n",
    "### 线性支持向量机\n",
    "假设训练数据集线性不可分，通常情况下，训练数据集可以分为少数特异点与其余的线性可分的样本点。\n",
    "\n",
    "对每个样本点$(x_i,y_i)$引入一个松弛变量$\\xi_i\\ge0$，使函数间隔加上松弛变量大于等于1。这样，约束条件变为\n",
    "$$\n",
    "y_i(w\\cdot x_i+b)\\ge 1-\\xi_i\n",
    "$$\n",
    "同时，目标函数由$\\frac{1}{2}||w||^2$变为\n",
    "$$\n",
    "\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i\n",
    "$$\n",
    "其中C>0称为惩罚参数\n",
    "**原始问题**\n",
    "$$\n",
    "\\min_{w,b,\\xi}\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i\\\\\n",
    "s.t.y_i(w\\cdot x_i+b)\\ge 1-\\xi_i\\quad i=1,2,\\cdots,N\\\\\n",
    "\\xi_i\\ge0,\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "### 学习的对偶算法\n",
    "原始问题的对偶问题是\n",
    "$$\n",
    "\\min_\\alpha\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i\\\\\n",
    "s.t.\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "0\\le\\alpha_i\\le C,\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "该对偶问题的拉格朗日函数为\n",
    "$$\n",
    "L(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i-\\sum_{i=1}^N\\alpha_i(y_i(w\\cdot x_i+b)-1+\\xi_i)-\\sum_{i=1}^N\\mu_i\\xi_i\n",
    "$$\n",
    "类似上文可得对偶问题。\n",
    "\n",
    "**线性支持向量机学习算法**\n",
    "\n",
    "输入：训练数据集$T=\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}$\n",
    "\n",
    "输出：分离超平面和分类决策函数\n",
    "\n",
    "(1)选择惩罚参数C>0，构造并求解凸二次规划问题\n",
    "$$\n",
    "\\min_\\alpha\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i\\\\\n",
    "s.t.\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "0\\le\\alpha_i\\le C,\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "求得最优解$\\alpha^*=(\\alpha_1^*,\\alpha_2^*,\\cdots,\\alpha_N^*)^T$\n",
    "\n",
    "(2)计算$w^*=\\sum_{i=1}^N\\alpha_i^*y_ix_i$\n",
    "\n",
    "选择$\\alpha^*$的一个分量$\\alpha^*_j$适合条件$0<\\alpha^*_j<C$，计算\n",
    "$$\n",
    "b^*=y_j-\\sum_{i=1}^Ny_i\\alpha_i^*(x_i\\cdot x_j)\n",
    "$$\n",
    "(3)求得分离超平面\n",
    "$$\n",
    "w^*\\cdot x+b^*=0\n",
    "$$\n",
    "分类决策函数\n",
    "$$\n",
    "f(x)=sign(w^*\\cdot x+b^*)\n",
    "$$\n",
    "### 支持向量\n",
    "若$0<\\alpha^*_i<C$,则$\\xi_i=0$，支持向量$x_i$恰好落在间隔边界上；\n",
    "\n",
    "若$\\alpha^*_i=C,0<\\xi_i<1$,，支持向量$x_i$分类正确；\n",
    "\n",
    "若$\\alpha^*_i=C,\\xi_i=1$，支持向量$x_i$恰好落在分离超平面上；\n",
    "\n",
    "若$\\alpha^*_i=C,\\xi_i>1$，支持向量$x_i$落在分离超平面误分一侧；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非线性支持向量机与核函数\n",
    "### 核技巧\n",
    "#### 非线性分类问题\n",
    "对线性分类方法求解非线性分类问题分为两步：\n",
    "\n",
    "首先使用一个变换将原空间的数据映射到新空间；\n",
    "\n",
    "然后再新空间里用线性分类学习方法从训练数据中学习分类模型。\n",
    "\n",
    "核技巧应用于支持向量机，其基本想法是通过一个非线性变换将输入空间（欧氏空间$R^n$或离散集合）对应于一个特征空间（希尔伯特空间），使得再输入空间中的超曲面模型对应于特征空间中的超平面模型。\n",
    "\n",
    "#### 核函数定义\n",
    "设X是输入空间（欧氏空间或离散集合），又设H为特征空间，如果存在一个从X到H的映射\n",
    "$$\n",
    "\\phi(x):X\\to H\n",
    "$$\n",
    "使得对所有$x,z\\in X$，函数K(x,z)满足条件\n",
    "$$\n",
    "K(x,z)=\\phi(x)\\cdot\\phi(z)\n",
    "$$\n",
    "则称K(x,z)为核函数，$\\phi(x)$为映射函数。\n",
    "\n",
    "核技巧的想法是在学习和预测中只定义核函数K(x,z)，而不显示地定义映射函数$\\phi(x)$。通常，直接计算核函数比较容易，通过映射函数计算核函数并不容易。\n",
    "\n",
    "注意，给定核函数，特征空间与映射函数的取法并不唯一。\n",
    "\n",
    "#### 核技巧在支持向量机中的应用\n",
    "记对偶问题的目标函数中的内积为$x_i\\cdot x_j$，可以用核函数$K(x_i,x_j)=\\phi(x_i)\\cdot\\phi(x_j)$来代替，此时对偶函数的目标函数成为\n",
    "$$\n",
    "W(\\alpha)=\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i, x_j)-\\sum_{i=1}^N\\alpha_i\n",
    "$$\n",
    "同样，分类决策函数为\n",
    "$$\n",
    "f(x)=sign(\\sum_{i=1}^N\\alpha_i^*y_iK(x_i,x)+b^*)\n",
    "$$\n",
    "### 正定核\n",
    "已知映射函数$\\phi$,可以通过内积求得核函数K(x,z),通过正定核条件可以在不知道映射函数来判断给定函数是否为核函数\n",
    "\n",
    "#### 定义映射，构成向量空间\n",
    "先定义映射\n",
    "$$\n",
    "\\phi :x\\to K(\\cdot,x)\n",
    "$$\n",
    "根据这一映射，对任意$x_i\\in X$，$\\alpha_i\\in R$，$i=1,2,\\cdots,m$，定义线性组合\n",
    "$$\n",
    "f(\\cdot)=\\sum_{i=1}^m\\alpha_iK(\\cdot,x_i)\n",
    "$$\n",
    "考虑由线性组合为元素的集合S，由于集合S对加法乘法封闭，S构成一个向量空间。\n",
    "#### 在S上定义内积\n",
    "在S上定义一个运算*,对任意$f,g\\in S$，\n",
    "$$\n",
    "f(\\cdot)=\\sum_{i=1}^m\\alpha_iK(\\cdot,x_i)\\\\\n",
    "g(\\cdot)=\\sum_{j=1}^l\\beta_jK(\\cdot,z_j)\n",
    "$$\n",
    "定义运算\n",
    "$$\n",
    "f*g(\\cdot) = f(\\cdot)=\\sum_{i=1}^m\\sum_{j=1}^l\\alpha_i\\beta_jK(x_i,z_j)\n",
    "$$\n",
    "#### 将内积空间S完备化为希尔伯特空间\n",
    "定义范数为\n",
    "$$\n",
    "||f||=\\sqrt{f\\cdot f}\n",
    "$$\n",
    "同时，由于核K满足再生性，即满足\n",
    "$$\n",
    "K(\\cdot,x)\\cdot f=f(x)\\\\\n",
    "K(\\cdot,x)\\cdot K(\\cdot,z)=K(x,z)\n",
    "$$\n",
    "该希尔伯特空间成为再生核希尔伯特空间，K成为再生核。\n",
    "#### 正定核的充要条件\n",
    "设$K:X\\times X\\to R$是对称函数，则K(x,z)为正定核函数的充要条件为对任意$x_i\\in X，i=1,2,\\cdots,m$，K(x,z)对应的Gram矩阵：\n",
    "$$\n",
    "K=[K(x_i,x_j)]_{m\\times m}\n",
    "$$\n",
    "是半正定矩阵\n",
    "### 常用核函数\n",
    "#### 多项式核函数\n",
    "$$\n",
    "K(x,z)=(x\\cdot z+1)^p\n",
    "$$\n",
    "#### 高斯核函数\n",
    "$$\n",
    "K(x,z)=exp\\left(-\\frac{||x-z||^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "### 非线性支持向量机\n",
    "**非线性支持向量机算法**\n",
    "输入：训练数据集$T={(x_i,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$\n",
    "\n",
    "输出：分类决策函数\n",
    "\n",
    "(1)选取适当的核函数K(x,z)和适当的参数C，构造并求解最优化函数\n",
    "$$\n",
    "\\min_{\\alpha}\\quad \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i, x_j)-\\sum_{i=1}^N\\alpha_i\\\\\n",
    "s.t.\\quad \\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    " 0\\le \\alpha_i \\le C\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "求得最优解$\\alpha^*=(\\alpha_1^*,\\alpha_2^*,\\cdots,\\alpha_N^*)^T$\n",
    "\n",
    "(2)选择$\\alpha^*$的一个分量$\\alpha^*_j$适合条件$0<\\alpha^*_j<C$，计算\n",
    "$$\n",
    "b^*=y_j-\\sum_{i=1}^Ny_i\\alpha_i^*K(x_i\\cdot x_j)\n",
    "$$\n",
    "\n",
    "(3)构造决策函数：\n",
    "$$\n",
    "f(x)=sign\\left(\\sum_{i=1}^N\\alpha_i^*y_iK(x\\cdot x_i)+b^* \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列最小最优化算法(SMO算法)\n",
    "SMO算法主要解如下凸二次规划的对偶问题：\n",
    "$$\n",
    "\\min_{\\alpha}\\quad \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i, x_j)-\\sum_{i=1}^N\\alpha_i\\\\\n",
    "s.t.\\quad \\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    " 0\\le \\alpha_i \\le C\\quad i=1,2,\\cdots,N\n",
    "$$\n",
    "SMO算法是一种启发式算法，其基本思路是：\n",
    "\n",
    "如果所有变量的解都满足此最优化问题的KKT条件，那么这个最优化问题的解就找到了，因为KKT条件是该最优化问题的充要条件。\n",
    "\n",
    "否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解。这时，子问题可以通过解析方法求解。\n",
    "\n",
    "子问题有两个变量，一个是违反KKT条件最严重的一个，另一个由约束条件自动确定。\n",
    "\n",
    "由已知条件$\\quad \\sum_{i=1}^N\\alpha_iy_i=0$得，若一个变量改变，另一个变量因之改变。\n",
    "### 两个变量二次规划求解方法\n",
    "假设选择的两个变量为$\\alpha_1,\\alpha_2$,于是SMO最优化子问题可以写成：\n",
    "$$\n",
    "\\min_{\\alpha_1,\\alpha_2}\\quad W(\\alpha_1,\\alpha_2)=\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2-(\\alpha_1+\\alpha_2)+y_1\\alpha_1\\sum_{i=3}^Ny_i\\alpha_iK_{i1}+y_2\\alpha_2\\sum_{i=3}^Ny_i\\alpha_iK_{i2}\\\\\n",
    "s.t.\\quad \\alpha_1y_1+\\alpha_2y_2=\\zeta\\\\\n",
    "0\\le \\alpha_i \\le C\\quad i=1,2\n",
    "$$\n",
    "其中$K_{ij}=K(x_i,x_j),i=1,2,\\cdots,N$,$\\zeta$为常数。\n",
    "\n",
    "假设问题的初始可行解为$\\alpha_1^{old},\\alpha_2^{old}$，最优解为$\\alpha_1^{new},\\alpha_2^{new}$，并且假设在沿着约束方向未经剪辑时$\\alpha_2^{new.unc}$.\n",
    "\n",
    "由于$\\alpha_2^{new}$需满足不等式约束，所以$\\alpha_2^{new}$的取值范围满足\n",
    "$$\n",
    "L\\le \\alpha_2^{new}\\le H\n",
    "$$\n",
    "其中L，H为$\\alpha^{new}_2$所在对角线段端点的界，\n",
    "\n",
    "若$y_1\\ne y_2$,则\n",
    "$$\n",
    "L=\\max(0,\\alpha_2^{old}-\\alpha_1^{old}),\\quad H =\\min(C,C+\\alpha_2^{old}-\\alpha_1^{old})\n",
    "$$\n",
    "若$y_1=y_2$,则\n",
    "$$\n",
    "L=\\max(0,\\alpha_2^{old}+\\alpha_1^{old}-C),\\quad H =\\min(C,\\alpha_2^{old}+\\alpha_1^{old})\n",
    "$$\n",
    "记$g(x)=\\sum_{i=1}^N\\alpha_iy_iK(x_i,x)+b$,\n",
    "\n",
    "令\n",
    "$$\n",
    "E_i=g(x_i)-y_i=\\left(\\sum_{j=1}^N\\alpha_jy_jK(x_j,x_i)+b\\right)-y_i\n",
    "$$\n",
    "则**最优化问题沿着约束方向未经剪辑时的解为**\n",
    "$$\n",
    "\\alpha_2^{new,unc}=\\alpha_2^{old}+\\frac{y_2(E_1-E_2)}{\\eta}\n",
    "$$\n",
    "其中$\\eta=K_{11}+K_{22}-2K_{12}=||\\phi(x_1)-\\phi(x_2)||^2$\n",
    "\n",
    "**经剪辑后$\\alpha_2$的解为**\n",
    "$$\n",
    "\\alpha_2^{new}=\n",
    "\\begin{cases}\n",
    "H, &if\\;\\alpha_2^{new.unc}>H\\\\\n",
    "\\alpha_2^{new.unc},&if\\;L\\le\\alpha_2^{new.unc}\\le H\\\\\n",
    "L,&if\\;\\alpha_2^{new.unc}<L\n",
    "\\end{cases}\n",
    "$$\n",
    "**由$\\alpha_2^{new}$求得$\\alpha_1^{new}$为**\n",
    "$$\n",
    "\\alpha_1^{new}=\\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new})\n",
    "$$\n",
    "### 变量的选择方法\n",
    "#### 第一个变量的选择\n",
    "SMO称选择的第一个变量的过程为外层循环。外层循环在训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第一个变量。具体地，检查训练样本点$(x_i,y_i)$是否满足KKT条件，即\n",
    "$$\n",
    "\\alpha_i=0\\Leftrightarrow y_ig(x_i)\\ge1\\\\\n",
    "0<\\alpha_i<C\\Leftrightarrow  y_ig(x_i)=1\\\\\n",
    "\\alpha_i=C\\Leftrightarrow  y_ig(x_i)=\\le1\n",
    "$$\n",
    "检验过程中，先遍历所有满足条件$0<\\alpha_i<C$的样本点，即在间隔边界的支持向量点，如果都满足KKT条件，则遍历整个训练集。\n",
    "#### 第二个变量的选择\n",
    "SMO称选择的第一个变量的过程为内层循环。假设在外层循环中找到第一个变量$\\alpha_1$,则第二个变量的选择标准是希望能使$\\alpha_2$由足够大的变化。\n",
    "\n",
    "已知$\\alpha_2^{new}$是依赖于$|E_1-E_2|$的。因此，一种简单的做法是选择$\\alpha_2$,是对应的$|E_1-E_2|$最大。\n",
    "\n",
    "特殊情况下，如果内存循环通过以上方法不能使目标函数有足够的下降，那么采用以下启发式规则。遍历在间隔边界上的支持向量点，一次将其对应的变量作为$\\alpha_2$试用，直到目标函数有足够的下降。若找不到则遍历训练数据集，若仍找不到，则放弃第一个$\\alpha_1$，再通过外层循环寻求另外的$\\alpha_1$。\n",
    "####  计算阈值b与插值$E_1$\n",
    "由KKT条件可知\n",
    "$$\n",
    "\\sum_{i=1}^N\\alpha_iy_iK_{i1}+b=y_1\n",
    "$$\n",
    "于是：\n",
    "$$\n",
    "b_1^{new}=y_1-sum_{i=1}^N\\alpha_iy_iK_{i1}-\\alpha_1^{new}y_1K_{11}-\\alpha_2^{new}y_2K_{21}\n",
    "$$\n",
    "由$E_1$的定义有\n",
    "$$\n",
    "E_1=\\sum_{i=3}^N\\alpha_iy_iK_{i1}+\\alpha_1^{old}y_1K_{11}+\\alpha_2^{old}y_2K_{21}+b^{old}-y_1\n",
    "$$\n",
    "可得：\n",
    "$$\n",
    "b_1^{new}=-E_1-y_1K_{12}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{22}(\\alpha_2^{new}-\\alpha_2^{old})+b^{old}\n",
    "$$\n",
    "若$0<\\alpha_2^{new}<C$，那么：\n",
    "$$\n",
    "b_2^{new}=-E_2-y_1K_{12}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{22}(\\alpha_2^{new}-\\alpha_2^{old})+b^{old}\n",
    "$$\n",
    "如果$\\alpha_1^{new},\\alpha_2^{new}$都满足条件$0<\\alpha_i^{new}<C,\\quad i=1,2$，那么$b_1^{new}=b_2^{new}$。如果$\\alpha_1^{new},\\alpha_2^{new}$是0或C，那么选择他们的中点作为$b^{new}$。\n",
    "\n",
    "此时，\n",
    "$$\n",
    "E_i^{new}=\\sum_Sy_j\\alpha_jK(x_i,x_j)+b^{new}-y_i\n",
    "$$\n",
    "其中，S为所有支持向量$x_j$的集合\n",
    "### SMO算法\n",
    "输入：训练数据集$T={(x_i,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，精度$\\epsilon$；\n",
    "\n",
    "输出：近似解$\\hat{\\alpha}$。\n",
    "\n",
    "(1)选取初值$\\alpha^{(0)}=0$，令k=1；\n",
    "\n",
    "(2)选取优化变量$\\alpha_1^{(k)},\\alpha_2^{(k)}$，解析求解最优化问题，求得最优解$\\alpha_1^{(k+1)},\\alpha_2^{(k+1)}$，更新$\\alpha$为$\\alpha^{(k+1)}$\n",
    "\n",
    "(3)若在精度$\\epsilon$范围内满足停机条件\n",
    "$$\n",
    "\\sum_{i=1}^Na_Iy_i=0\\\\\n",
    "0\\le\\alpha_i\\le C\\quad i=1,2,\\cdots,N\\\\\n",
    "y_I\\cdot g(x_i)=\n",
    "\\begin{cases}\n",
    "\\ge 1,&\\;\\{x_i|\\alpha_i=0\\}\\\\\n",
    "= 1,&\\;\\{x_i|0<\\alpha_i<C\\}\\\\\n",
    "\\le 1,&\\;\\{x_i|\\alpha_i=C\\}\n",
    "\\end{cases}\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "g(x_i)=\\sum_{j=1}^N\\alpha_jy_jK(x_j,x_i)+b\n",
    "$$\n",
    "则转(4)，否则令k=k+1，转(2)\n",
    "\n",
    "(4)令$\\hat{\\alpha}=\\alpha^{(k+1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
