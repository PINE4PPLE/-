{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "## 决策树模型与学习\n",
    "### 决策树模型\n",
    "决策树由结点和有向边组成；结点有两种类型：内部节点和叶节点，内部节点表示一个特征或者属性，叶节点表示一个类。\n",
    "\n",
    "用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到子节点；这时每个子节点对应着特征的一个取值，如此递归对实例进行测试，直至到达叶节点，最后将实例分至叶节点的类中。\n",
    "\n",
    "### 决策树与if-then规则\n",
    "决策树可以看作一个If-then规则的集合。从决策树的根结点到叶结点的每一条路径构建一条规则。路径上内部节点的特征对应着规则的条件，叶节点则对应着结论。决策树对应的if-then规则集合互斥且完备，意思是每一个实例都被且只被一条路径或一条规则覆盖。\n",
    "\n",
    "### 决策树与条件概率分布\n",
    "决策树表示特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上。将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成一个条件概率分布。决策树的一个路径对应一个单元。\n",
    "\n",
    "### 决策树学习\n",
    "假定给定数据集$D=\\{(x_1, y_1),(x_2,y_2)\\cdots,(x_N,y_N)\\}$,其中，$x_i=(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})^T$为输入实例(特征向量)，n为特征个数，$y_i\\in\\{1,2,\\cdots,K\\}$为类标记。\n",
    "\n",
    "学习过程包括特征选择，决策树的生成，决策树的剪枝。\n",
    "\n",
    "决策树学习本质上是从数据集中归纳出一组分类规则，与训练集不冲突的决策树可能有多个，也可能一个没有，我们需要一个与训练集数据矛盾较小的决策树同时也具有较好的泛化能力。\n",
    "\n",
    "决策树的损失函数通常为正则化的极大似然函数，学习策略是最小化损失函数。然而从所有的决策树中选取损失函数最小的决策树（最优决策树）是NP完全问题，因此学习算法通常采用启发式方法，近似求解这一问题，这样得到的决策树是次优的。\n",
    "\n",
    "决策树的学习算法通常是递归地选取最优特征，并根据该特征对数据集进行分割，使得对各个数据集有一个最好的分类的过程。这一过程对应着特征空间的划分也对应着决策树的构建过程。开始构建根结点，将所有数据结点放在根结点；选择一个最优特征，按照这一特征将训练数据集划分为子集，使各个子集有在当前条件下最好的分类，如果这些子集已经能够能被基本正确分类，则构建叶结点。如此递归就得到一个决策树。\n",
    "\n",
    "以上方法构建的决策树可能有很好的分类能力，但泛化能力不一定强，换言之可能发生过拟合现象。我们需要对已生成的树进行从下而上的剪枝，具体地，就是去掉过于细分的叶结点，使其退回至父结点乃至更高的结点，然后将父结点或更高的结点改为叶结点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择\n",
    "\n",
    "特征选择在于选取对训练数据具有分类能力的特征，选择的基准是信息增益或信息增益比。\n",
    "\n",
    "### 信息增益\n",
    "\n",
    "为了便于说明先给出熵与条件熵的定义。\n",
    "\n",
    "熵是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为\n",
    "$$\n",
    "P(X=x_i)=p_i,\\qquad i=1,2,\\cdots,n\n",
    "$$\n",
    "则随机变量X的熵定义为\n",
    "$$\n",
    "H(X)=-\\sum_{i=1}^np_i\\operatorname{log}p_i\n",
    "$$\n",
    "若$p_i=0$则定义$p_i\\operatorname{log}p_i=0$通常，对数取2或e为底，这是熵的单位称为比特或纳特。\n",
    "\n",
    "设有随机变量（X，Y），其联合分布概率为\n",
    "$$\n",
    "P(X=x_i, Y=y_j)=p_{ij},\\qquad i=1,2,\\cdots,n\\quad j=1,2,\\cdots,m\n",
    "$$\n",
    "条件熵H（Y|X）表示随机变量X在给定的条件下随机变量Y的不确定性。则随机变量X在给定的条件下随机变量Y的条件熵H（Y|X）定义为\n",
    "$$\n",
    "H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i)\n",
    "$$\n",
    "这里$p_i=P(X=x_i)$.\n",
    "\n",
    "信息增益表示得知特征X的信息而使类Y的信息的不确定性减少的程度。\n",
    "\n",
    "特征A对训练数据集D的信息增益g(D|A)定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即\n",
    "$$\n",
    "g(D,A)=H(D)-H(D|A)\n",
    "$$\n",
    "一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息，决策树中信息增益等价于训练数据集中类与特征的互信息。\n",
    "\n",
    "根据信息增益准则的特征选择方法是：对训练数据集D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益大的特征。\n",
    "\n",
    "设训练数据集为D，|D|表示其样本容量，即样本个数。设有K个类$C_k$,特征A有n个不同的取值$\\{a_1.a_2,\\cdots,a_n\\}$,根据A的取值将D划分为n个子集，$D_1,D_2,\\cdots,D_n$，记子集$D_i$中属于类$C_k$的样本集合为$D_{ij}$,于是信息增益算法如下：\n",
    "\n",
    "输入：训练数据集D和特征A\n",
    "\n",
    "输出：特征A对D的信息增益g(D,A)\n",
    "\n",
    "(1)计算数据集的经验熵H(D)\n",
    "$$\n",
    "H(D)=-\\sum_{k=1}^K\\frac{|C_k|}{|D|}\\operatorname{log}_2\\frac{|C_k|}{|D|}\n",
    "$$\n",
    "(2)计算特征A对数据集的经验条件熵H(D|A)\n",
    "$$\n",
    "H(D|A)=\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}\\operatorname{log}_2\\frac{|D_{ik}|}{|D_i|}\n",
    "$$\n",
    "(3)计算信息增益\n",
    "$$\n",
    "g(D,A)=H(D)-H(D|A)\n",
    "$$\n",
    "### 信息增益比\n",
    "信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大，使用信息增益比可以对这一问题进行校正。\n",
    "特征A对数据训练集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比。\n",
    "$$\n",
    "g_R(D,A)=\\frac{g(D,A)}{H(D)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的生成\n",
    "### ID3算法\n",
    "ID3算法的核心时在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。\n",
    "\n",
    "算法：\n",
    "\n",
    "输入：训练数据集D，特征集A，阈值$\\epsilon$\n",
    "\n",
    "输出：决策树T\n",
    "\n",
    "(1)若D中所有实例属于同一类$C_k$,则T为单结点树，并将$C_k$作为给节点的标记。返回T。\n",
    "\n",
    "(2)若$A=\\phi$,则T为单结点树，并将D中实例数最大的类$C_k$作为该节点的标记，返回T。\n",
    "\n",
    "(3)否则，根据算法计算A中各特征对D的信息增益。选择信息增益最大的特征$A_g$\n",
    "\n",
    "(4))若$A_g$中信息增益小于$\\epsilon$,则T为单结点树，并将D中实例数最大的类$C_k$作为该节点的标记，返回T。\n",
    "\n",
    "(5)否则，对$A_g$的每一个可能$a_g$,依$A_g=a_g$将D分割为非空子集$D_i$,将$D_i$中实例数最大的类$C_k$作为该节点的标记，构建子节点，由结点及其子结点构成树T，返回T。\n",
    "\n",
    "(6)对dii个子结点，以$D_i$为训练集，以$A-{A-g}$为特征集，递归调用(1)-(5),得到子树$T_i$,返回$T_i$。\n",
    "### C4.5算法算法：\n",
    "相对于ID3算法，C4.5算法选择信息增益比作为特征选择的标准。\n",
    "\n",
    "输入：训练数据集D，特征集A，阈值$\\epsilon$\n",
    "\n",
    "输出：决策树T\n",
    "\n",
    "(1)若D中所有实例属于同一类$C_k$,则T为单结点树，并将$C_k$作为给节点的标记。返回T。\n",
    "\n",
    "(2)若$A=\\phi$,则T为单结点树，并将D中实例数最大的类$C_k$作为该节点的标记，返回T。\n",
    "\n",
    "(3)否则，根据算法计算A中各特征对D的信息增益比。选择信息增益最大的特征$A_g$\n",
    "\n",
    "(4))若$A_g$中信息增益比小于$\\epsilon$,则T为单结点树，并将D中实例数最大的类$C_k$作为该节点的标记，返回T。\n",
    "\n",
    "(5)否则，对$A_g$的每一个可能$a_g$,依$A_g=a_g$将D分割为非空子集$D_i$,将$D_i$中实例数最大的类$C_k$作为该节点的标记，构建子节点，由结点及其子结点构成树T，返回T。\n",
    "\n",
    "(6)对dii个子结点，以$D_i$为训练集，以$A-{A-g}$为特征集，递归调用(1)-(5),得到子树$T_i$,返回$T_i$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的剪枝\n",
    "决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树T的叶结点个数为|T|，t是树T的叶结点，该节点有$T_t$个样本点，其中k类的样本点有$N_{tk}$个，$H_t(T)$为叶结点t上的经验熵，$\\alpha\\ge0$为参数，则决策树学习的损失函数可以定义为\n",
    "$$\n",
    "C_{\\alpha}(T)=\\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha|T|\n",
    "$$\n",
    "其中，经验熵为\n",
    "$$\n",
    "H_t(T)=-\\sum_{k}\\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "若将损失函数右侧第一项定义为\n",
    "$$\n",
    "C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)=-\\sum_{t=1}^{|T|}\\sum_{k}^KN_{tk}log\\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "这时有\n",
    "$$\n",
    "C_{\\alpha}(T)=C(T)+\\alpha|T|\n",
    "$$\n",
    "这里损失函数可以看作由衡量模型对训练数据集的拟合程度与泛化能力组成。$\\alpha|T|$类似于之前损失函数中的惩罚项。\n",
    "\n",
    "树的剪枝算法：\n",
    "\n",
    "输入：生成算法产生的整个树T，参数$\\alpha$。\n",
    "\n",
    "输出：修建后的树$T_{\\alpha}$。\n",
    "\n",
    "(1)计算每个结点的经验熵。\n",
    "\n",
    "(2)递归地从树的叶结点向上回缩。设一组叶结点回缩到其父结点之前和之后整体树分别为$T_B$$T_A$,对应的损失函数的值分别是$C_{\\alpha}(T_A)$与$C_{\\alpha}(T_B)$，如果\n",
    "$$\n",
    "C_{\\alpha}(T_A)\\le C_{\\alpha}(T_B)\n",
    "$$\n",
    "则进行剪枝，将父结点变为新的叶结点。\n",
    "\n",
    "(3)返回(2)，直至不能继续为止，得到损失函数最小的子树$T_{\\alpha}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART算法\n",
    "分类与回归树"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
