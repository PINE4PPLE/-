{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "## 决策树模型与学习\n",
    "### 决策树模型\n",
    "决策树由结点和有向边组成；结点有两种类型：内部节点和叶节点，内部节点表示一个特征或者属性，叶节点表示一个类。\n",
    "\n",
    "用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到子节点；这时每个子节点对应着特征的一个取值，如此递归对实例进行测试，直至到达叶节点，最后将实例分至叶节点的类中。\n",
    "\n",
    "### 决策树与if-then规则\n",
    "决策树可以看作一个If-then规则的集合。从决策树的根结点到叶结点的每一条路径构建一条规则。路径上内部节点的特征对应着规则的条件，叶节点则对应着结论。决策树对应的if-then规则集合互斥且完备，意思是每一个实例都被且只被一条路径或一条规则覆盖。\n",
    "\n",
    "### 决策树与条件概率分布\n",
    "决策树表示特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上。将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成一个条件概率分布。决策树的一个路径对应一个单元。\n",
    "\n",
    "### 决策树学习\n",
    "假定给定数据集$D=\\{(x_1, y_1),(x_2,y_2)\\cdots,(x_N,y_N)\\}$,其中，$x_i=(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})^T$为输入实例(特征向量)，n为特征个数，$y_i\\in\\{1,2,\\cdots,K\\}$为类标记。\n",
    "\n",
    "学习过程包括特征选择，决策树的生成，决策树的剪枝。\n",
    "\n",
    "决策树学习本质上是从数据集中归纳出一组分类规则，与训练集不冲突的决策树可能有多个，也可能一个没有，我们需要一个与训练集数据矛盾较小的决策树同时也具有较好的泛化能力。\n",
    "\n",
    "决策树的损失函数通常为正则化的极大似然函数，学习策略是最小化损失函数。然而从所有的决策树中选取损失函数最小的决策树（最优决策树）是NP完全问题，因此学习算法通常采用启发式方法，近似求解这一问题，这样得到的决策树是次优的。\n",
    "\n",
    "决策树的学习算法通常是递归地选取最优特征，并根据该特征对数据集进行分割，使得对各个数据集有一个最好的分类的过程。这一过程对应着特征空间的划分也对应着决策树的构建过程。开始构建根结点，将所有数据结点放在根结点；选择一个最优特征，按照这一特征将训练数据集划分为子集，使各个子集有在当前条件下最好的分类，如果这些子集已经能够能被基本正确分类，则构建叶结点。如此递归就得到一个决策树。\n",
    "\n",
    "以上方法构建的决策树可能有很好的分类能力，但泛化能力不一定强，换言之可能发生过拟合现象。我们需要对已生成的树进行从下而上的剪枝，具体地，就是去掉过于细分的叶结点，使其退回至父结点乃至更高的结点，然后将父结点或更高的结点改为叶结点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择\n",
    "\n",
    "特征选择在于选取对训练数据具有分类能力的特征，选择的基准是信息增益或信息增益比。\n",
    "\n",
    "### 信息增益\n",
    "\n",
    "为了便于说明先给出熵与条件熵的定义。\n",
    "\n",
    "熵是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为\n",
    "$$\n",
    "P(X=x_i)=p_i,\\qquad i=1,2,\\cdots,n\n",
    "$$\n",
    "则随机变量X的熵定义为\n",
    "$$\n",
    "H(X)=-\\sum_{i=1}^np_i\\operatorname{log}p_i\n",
    "$$\n",
    "若$p_i=0$则定义$p_i\\operatorname{log}p_i=0$通常，对数取2或e为底，这是熵的单位称为比特或纳特。\n",
    "\n",
    "设有随机变量（X，Y），其联合分布概率为\n",
    "$$\n",
    "P(X=x_i, Y=y_j)=p_{ij},\\qquad i=1,2,\\cdots,n\\quad j=1,2,\\cdots,m\n",
    "$$\n",
    "条件熵H（Y|X）表示随机变量X在给定的条件下随机变量Y的不确定性。则随机变量X在给定的条件下随机变量Y的条件熵H（Y|X）定义为\n",
    "$$\n",
    "H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i)\n",
    "$$\n",
    "这里$p_i=P(X=x_i)$.\n",
    "\n",
    "信息增益表示得知特征X的信息而使类Y的信息的不确定性减少的程度。\n",
    "\n",
    "特征A对训练数据集D的信息增益g(D|A)定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即\n",
    "$$\n",
    "g(D,A)=H(D)-H(D|A)\n",
    "$$\n",
    "一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息，决策树中信息增益等价于训练数据集中类与特征的互信息。\n",
    "\n",
    "根据信息增益准则的特征选择方法是：对训练数据集D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益大的特征。\n",
    "\n",
    "设训练数据集为D，|D|表示其样本容量，即样本个数。设有K个类$C_k$,特征A有n个不同的取值$\\{a_1.a_2,\\cdots,a_n\\}$,根据A的取值将D划分为n个子集，$D_1,D_2,\\cdots,D_n$，记子集$D_i$中属于类$C_k$的样本集合为$D_{ij}$,于是信息增益算法如下：\n",
    "\n",
    "输入：训练数据集D和特征A\n",
    "\n",
    "输出：特征A对D的信息增益g(D,A)\n",
    "\n",
    "(1)计算数据集的经验熵H(D)\n",
    "$$\n",
    "H(D)=-\\sum_{k=1}^K\\frac{|C_k|}{|D|}\\operatorname{log}_2\\frac{|C_k|}{|D|}\n",
    "$$\n",
    "(2)计算特征A对数据集的经验条件熵H(D|A)\n",
    "$$\n",
    "H(D|A)=\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}\\operatorname{log}_2\\frac{|D_{ik}|}{|D_i|}\n",
    "$$\n",
    "(3)计算信息增益\n",
    "$$\n",
    "g(D,A)=H(D)-H(D|A)\n",
    "$$\n",
    "### 信息增益比\n",
    "信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大，使用信息增益比可以对这一问题进行校正。\n",
    "特征A对数据训练集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比。\n",
    "$$\n",
    "g_R(D,A)=\\frac{g(D,A)}{H(D)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的生成\n",
    "### ID3算法\n",
    "ID3算法的核心时在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。\n",
    "\n",
    "算法：\n",
    "\n",
    "输入：训练数据集D，特征集A，阈值$\\epsilon$\n",
    "\n",
    "输出：决策树T\n",
    "\n",
    "(1)若D中所有实例属于同一类$C_k$,则T为单结点树，并将$C_k$作为给节点的标记。返回T。\n",
    "\n",
    "(2)若$A=\\phi$,则T为单结点树，并将D中实例数最大的类$C_k$作为该节点的标记，返回T。\n",
    "\n",
    "(3)否则，根据算法计算A中各特征对D的信息增益。选择信息增益最大的特征$A_g$\n",
    "\n",
    "(4))若$A_g$中信息增益小于$\\epsilon$,则T为单结点树，并将D中实例数最大的类$C_k$作为该节点的标记，返回T。\n",
    "\n",
    "(5)否则，对$A_g$的每一个可能$a_g$,依$A_g=a_g$将D分割为非空子集$D_i$,将$D_i$中实例数最大的类$C_k$作为该节点的标记，构建子节点，由结点及其子结点构成树T，返回T。\n",
    "\n",
    "(6)对dii个子结点，以$D_i$为训练集，以$A-{A-g}$为特征集，递归调用(1)-(5),得到子树$T_i$,返回$T_i$。\n",
    "### C4.5算法算法：\n",
    "相对于ID3算法，C4.5算法选择信息增益比作为特征选择的标准。\n",
    "\n",
    "输入：训练数据集D，特征集A，阈值$\\epsilon$\n",
    "\n",
    "输出：决策树T\n",
    "\n",
    "(1)若D中所有实例属于同一类$C_k$,则T为单结点树，并将$C_k$作为给节点的标记。返回T。\n",
    "\n",
    "(2)若$A=\\phi$,则T为单结点树，并将D中实例数最大的类$C_k$作为该节点的标记，返回T。\n",
    "\n",
    "(3)否则，根据算法计算A中各特征对D的信息增益比。选择信息增益最大的特征$A_g$\n",
    "\n",
    "(4))若$A_g$中信息增益比小于$\\epsilon$,则T为单结点树，并将D中实例数最大的类$C_k$作为该节点的标记，返回T。\n",
    "\n",
    "(5)否则，对$A_g$的每一个可能$a_g$,依$A_g=a_g$将D分割为非空子集$D_i$,将$D_i$中实例数最大的类$C_k$作为该节点的标记，构建子节点，由结点及其子结点构成树T，返回T。\n",
    "\n",
    "(6)对第i个子结点，以$D_i$为训练集，以$A-{A-g}$为特征集，递归调用(1)-(5),得到子树$T_i$,返回$T_i$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的剪枝\n",
    "决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树T的叶结点个数为|T|，t是树T的叶结点，该节点有$T_t$个样本点，其中k类的样本点有$N_{tk}$个，$H_t(T)$为叶结点t上的经验熵，$\\alpha\\ge0$为参数，则决策树学习的损失函数可以定义为\n",
    "$$\n",
    "C_{\\alpha}(T)=\\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha|T|\n",
    "$$\n",
    "其中，经验熵为\n",
    "$$\n",
    "H_t(T)=-\\sum_{k}\\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "若将损失函数右侧第一项定义为\n",
    "$$\n",
    "C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)=-\\sum_{t=1}^{|T|}\\sum_{k}^KN_{tk}log\\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "这时有\n",
    "$$\n",
    "C_{\\alpha}(T)=C(T)+\\alpha|T|\n",
    "$$\n",
    "这里损失函数可以看作由衡量模型对训练数据集的拟合程度与泛化能力组成。$\\alpha|T|$类似于之前损失函数中的惩罚项。\n",
    "\n",
    "树的剪枝算法：\n",
    "\n",
    "输入：生成算法产生的整个树T，参数$\\alpha$。\n",
    "\n",
    "输出：修建后的树$T_{\\alpha}$。\n",
    "\n",
    "(1)计算每个结点的经验熵。\n",
    "\n",
    "(2)递归地从树的叶结点向上回缩。设一组叶结点回缩到其父结点之前和之后整体树分别为$T_B$$T_A$,对应的损失函数的值分别是$C_{\\alpha}(T_A)$与$C_{\\alpha}(T_B)$，如果\n",
    "$$\n",
    "C_{\\alpha}(T_A)\\le C_{\\alpha}(T_B)\n",
    "$$\n",
    "则进行剪枝，将父结点变为新的叶结点。\n",
    "\n",
    "(3)返回(2)，直至不能继续为止，得到损失函数最小的子树$T_{\\alpha}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#决策树结点\n",
    "class Node:\n",
    "    def __init__(self, label=None, feature_idx=None, feature_vals=None):\n",
    "        \"\"\"\n",
    "        label: label不为空，则说明该结点为叶结点，yield测时返回label值\n",
    "        feature_idx：决策树结点所用的特征索引\n",
    "        feature_vals：决策树特征索引对应的取值\n",
    "        tree:结点的子树\n",
    "        \"\"\"\n",
    "        self.label = label\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_vals = feature_vals\n",
    "        self.tree = {}\n",
    "        \n",
    "    def add_node(self, val, node):\n",
    "        self.tree[val] = node\n",
    "\n",
    "    def predict(self, features):\n",
    "        if not self.label:\n",
    "            return self.tree[features[self.feature]].predict(features)\n",
    "        return self.label\n",
    "\n",
    "class DTree:\n",
    "    def __init__(self, ep):\n",
    "        self.ep = ep\n",
    "        self._tree = tree\n",
    "    #经验熵\n",
    "    @staticmethod\n",
    "    def entropy(datasets):\n",
    "        data_length = len(datasets)\n",
    "        label_count = {}\n",
    "        for i in range(data_length):\n",
    "            label = datasets[i][-1]\n",
    "            if label not in label_count:\n",
    "                label_count[label] = 0\n",
    "            label_count[label] += 1\n",
    "        ent = -sum([(p / data_length) * log(p / data_length, 2)\n",
    "                    for p in label_count.values()])\n",
    "        return ent\n",
    "\n",
    "    # 经验条件熵\n",
    "    def cond_entropy(self, datasets, axis=0):\n",
    "        data_length = len(datasets)\n",
    "        feature_sets = {}\n",
    "        for i in range(data_length):\n",
    "            feature = datasets[i][axis]\n",
    "            if feature not in feature_sets:\n",
    "                feature_sets[feature] = []\n",
    "            feature_sets[feature].append(datasets[i])\n",
    "        ent = sum([(len(p) / data_length) * self.entropy(p)\n",
    "                        for p in feature_sets.values()])\n",
    "        return ent\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_gain(ent, cond_ent):\n",
    "        return ent - cond_ent\n",
    "    \n",
    "    def info_gain_train(self, datasets):\n",
    "        count = len(datasets[0]) - 1\n",
    "        ent = self.entropy(datasets)\n",
    "        best_feature = []\n",
    "        for c in range(count):\n",
    "            c_info_gain = self.info_gain(ent, self.cond_ent(datasets, axis=c))\n",
    "            best_feature.append((c, c_info_gain))\n",
    "        # 比较大小\n",
    "        best_ = max(best_feature, key=lambda x: x[-1])\n",
    "        return best_\n",
    "    \n",
    "    def train(self, train_data):\n",
    "        \"\"\"\n",
    "        input:数据集D(DataFrame格式)，特征集A，阈值e\n",
    "        output:决策树T\n",
    "        \"\"\"\n",
    "        _, y_train, features = train_data.iloc[:, :-1], train_data.iloc[:,-1], train_data.columns[:-1]\n",
    "        \n",
    "        # 1,若D中实例属于同一类Ck，则T为单节点树，并将类Ck作为结点的类标记，返回T\n",
    "        if len(y_train.value_counts()) == 1:\n",
    "            return Node(label=y_train.iloc[0])\n",
    "\n",
    "        # 2, 若A为空，则T为单节点树，将D中实例树最大的类Ck作为该节点的类标记，返回T\n",
    "        if len(features) == 0:\n",
    "            return Node(label=y_train.value_counts().sort_values(ascending=False).index[0])\n",
    "\n",
    "        # 3,计算最大信息增益 同5.1,Ag为信息增益最大的特征\n",
    "        max_feature, max_info_gain = self.info_gain_train(np.array(train_data))\n",
    "        max_feature_name = features[max_feature]\n",
    "\n",
    "        # 4,Ag的信息增益小于阈值eta,则置T为单节点树，并将D中是实例数最大的类Ck作为该节点的类标记，返回T\n",
    "        if max_info_gain < self.e:\n",
    "            return Node(label=y_train.value_counts().sort_values(ascending=False).index[0])\n",
    "\n",
    "        # 5,构建Ag子集\n",
    "        node_tree = Node(feature_name=max_feature_name, feature=max_feature)\n",
    "\n",
    "        feature_list = train_data[max_feature_name].value_counts().index\n",
    "        for f in feature_list:\n",
    "            sub_train_df = train_data.loc[train_data[max_feature_name] ==f].drop([max_feature_name], axis=1)\n",
    "\n",
    "            # 6, 递归生成树\n",
    "            sub_tree = self.train(sub_train_df)\n",
    "            node_tree.add_node(f, sub_tree)\n",
    "\n",
    "        # pprint.pprint(node_tree.tree)\n",
    "        return node_tree\n",
    "        \n",
    "    def fit(self, train_data):\n",
    "        self._tree = self.train(train_data)\n",
    "        return self._tree\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self._tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART算法\n",
    "分类与回归树（classification and regression tree）是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART假设决策树是二叉树,内部结点特征取值为“是”和“否”，左分支为“是”的分支，右分支为“否”的分支。\n",
    "\n",
    "## CART生成\n",
    "### 回归树的生成\n",
    "假设X，Y分别为输入变量和输出变量，并且Y是连续变量，给定数据集$D=\\{(x_1, y_1),(x_2,y_2)\\cdots,(x_N,y_N)\\}$.\n",
    "\n",
    "一个回归树对应着输入空间的一个划分以及划分空间上对应的输出值。假设已将输入空间划分为M个单元$R_1,R_2,\\cdots,R_M$，并在每个单元$R_m$上有一个固定的输出值$c_m$,于是2回归模型可以表示为\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^Mc_mI(x\\in R_m)\n",
    "$$\n",
    "当输入空间确定是，可以用平方误差$\\sum_{x_i\\in R_m}(y_i-f(x_i))^2$来表示回归树的训练误差。易知，单元$R_m$上的$c_m$的最优值$\\hat{c_m}$为该单元上实例对应的输出y的均值。\n",
    "\n",
    "在对输入空间进行划分时，采用启发式的方法，即选择第j个变量$x^{(j)}$和它的值s作为切分变量和切分点，并定义两个区域\n",
    "$$\n",
    "R_1(j,s)=\\{x|x^{(j)}\\le s\\} \\qquad R_2(j,s)=\\{x|x^{(j)}>s\\} \n",
    "$$\n",
    "然后，求解\n",
    "$$\n",
    "\\min_{j,s}[\\min_{c_1}\\sum_{x_i\\in R_{1(j,s)}}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i\\in R{_2(j,s)}}(y_i-c_2)^2]\n",
    "$$\n",
    "在通过求平均值得到两个区域的最优输出值。\n",
    "### 分类树生成\n",
    "设有K个类，样本点属于第k类的概率为$p_k$,则概率分布的基尼指数定义为：\n",
    "$$\n",
    "Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2\n",
    "$$\n",
    "对于给定样本集合D，其基尼指数为\n",
    "$$\n",
    "Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2\n",
    "$$\n",
    "其中，$C_k$为D中属于第k类的样本子集，K为类的个数。\n",
    "\n",
    "如果样本集合D根据特征A是否取某一可能值a被分为$D_1$和$D_2$两部分，即\n",
    "$$\n",
    "D_1=\\{(x,y)\\in D|A(x)=a\\},\\qquad D_2=D-D_1\n",
    "$$\n",
    "则在特征A的条件下，集合D的基尼系数为\n",
    "$$\n",
    "Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对决策树的一点补充\n",
    "##### 主要参考博客https://www.cnblogs.com/nxld/p/6371453.html\n",
    "### 决策树ID3算法的不足\n",
    "ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。　　\n",
    "\n",
    "       a)ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。\n",
    "\n",
    "       b)ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。如果校正这个问题呢？\n",
    "\n",
    "　　　　c) ID3算法对于缺失值的情况没有做考虑\n",
    "\n",
    "　　　　d) 没有考虑过拟合的问题\n",
    "### 决策树C4.5算法的改进\n",
    "      对于第一个问题，不能处理连续特征， C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,ama1,a2,...,am,则C4.5取相邻两样本值的中位数，一共取得m-1个划分点，其中第i个划分点Ti表示Ti表示为：Ti=ai+ai+12Ti=ai+ai+12。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为atat,则小于atat的值为类别1，大于atat的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。\n",
    "\n",
    "　　　  对于第二个问题，信息增益作为标准容易偏向于取值较多的特征的问题。我们引入一个信息增益比的变量IR(X,Y)IR(X,Y)，它是信息增益和特征熵的比值。\n",
    "    \n",
    "      对于第三个缺失值处理的问题，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。\n",
    "\n",
    "      对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。\n",
    "\n",
    "      对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。\n",
    "\n",
    "　　　　对于第4个问题，C4.5引入了正则化系数进行初步的剪枝。\n",
    "### 决策树C4.5算法的不足与思考\n",
    "      1)由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。\n",
    "\n",
    "　　　　2)C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。\n",
    "\n",
    "　　　　3)C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。\n",
    "\n",
    "　　　　4)C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。\n",
    "### 决策树CART算法的不足与思考\n",
    "      1）应该大家有注意到，无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样绝息到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1，这里不多介绍。\n",
    "\n",
    "　　　　2）如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
